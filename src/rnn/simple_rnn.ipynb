{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    spear_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(spear_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_char = '<.>'\n",
    "bow = set(spear_data.split() + [end_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25671\n"
     ]
    }
   ],
   "source": [
    "print(len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ind = { word: i for i, word in enumerate(bow) }\n",
    "ind_to_word = { i: word for i, word in enumerate(bow) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25671"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spear_data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_training_data = []\n",
    "for train in training_data:\n",
    "    # print(train)\n",
    "    if len(train) > 0 and train[len(train) - 1] != ':':\n",
    "        mod_training_data.append(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Before we proceed any further, hear me speak.',\n",
       " 'Speak, speak.',\n",
       " 'You are all resolved rather to die than to famish?',\n",
       " 'Resolved. resolved.',\n",
       " 'First, you know Caius Marcius is chief enemy to the people.',\n",
       " \"We know't, we know't.\",\n",
       " \"Let us kill him, and we'll have corn at our own price.\",\n",
       " \"Is't a verdict?\",\n",
       " \"No more talking on't; let it be done: away, away!\",\n",
       " 'One word, good citizens.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mod_training_data))\n",
    "mod_training_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # context length\n",
    "X, Y = [], []\n",
    "\n",
    "for sentence in mod_training_data:\n",
    "    context = [word_to_ind[end_char]] * block_size\n",
    "    \n",
    "    # print(sentence.split())\n",
    "    for word in sentence.split() + [end_char]:\n",
    "        ix = word_to_ind[word]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # crop the first latter and append the one ahead\n",
    "        context = context[1:] + [ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>']\n",
      "Target:  ['Before']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'Before']\n",
      "Target:  ['we']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'Before', 'we']\n",
      "Target:  ['proceed']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', 'Before', 'we', 'proceed']\n",
      "Target:  ['any']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', 'Before', 'we', 'proceed', 'any']\n",
      "Target:  ['further,']\n",
      "Input:  ['<.>', '<.>', '<.>', 'Before', 'we', 'proceed', 'any', 'further,']\n",
      "Target:  ['hear']\n",
      "Input:  ['<.>', '<.>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear']\n",
      "Target:  ['me']\n",
      "Input:  ['<.>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me']\n",
      "Target:  ['speak.']\n",
      "Input:  ['Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.']\n",
      "Target:  ['<.>']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>']\n",
      "Target:  ['Speak,']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'Speak,']\n",
      "Target:  ['speak.']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'Speak,', 'speak.']\n",
      "Target:  ['<.>']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>']\n",
      "Target:  ['You']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'You']\n",
      "Target:  ['are']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', '<.>', 'You', 'are']\n",
      "Target:  ['all']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', '<.>', 'You', 'are', 'all']\n",
      "Target:  ['resolved']\n",
      "Input:  ['<.>', '<.>', '<.>', '<.>', 'You', 'are', 'all', 'resolved']\n",
      "Target:  ['rather']\n",
      "Input:  ['<.>', '<.>', '<.>', 'You', 'are', 'all', 'resolved', 'rather']\n",
      "Target:  ['to']\n",
      "Input:  ['<.>', '<.>', 'You', 'are', 'all', 'resolved', 'rather', 'to']\n",
      "Target:  ['die']\n",
      "Input:  ['<.>', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die']\n",
      "Target:  ['than']\n"
     ]
    }
   ],
   "source": [
    "for input, target in zip(X[:20], Y[:20]):\n",
    "    print('Input: ', [ind_to_word[ind] for ind in input])\n",
    "    print('Target: ', [ind_to_word[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25671, 10])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.randn((len(bow), 10))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([204879, 8]), torch.Size([204879]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "vocab_size = len(bow)\n",
    "# B, T, C = 64, 8, 10\n",
    "\n",
    "Tx = X.shape[0]\n",
    "Ty = Y.shape[0]\n",
    "\n",
    "a0 = torch.zeros((1, 10))\n",
    "Wax = torch.randn((10, 10))\n",
    "ba = torch.randn(10)\n",
    "Waa = torch.randn((10, 10))\n",
    "\n",
    "Wya = torch.randn((10, vocab_size))\n",
    "by = torch.randn(vocab_size)\n",
    "\n",
    "parameters = [a0, Wax, ba, Waa, Wya, by]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 => Loss: 116.14977264404297\n",
      "For 10 => Loss: 102.89419555664062\n",
      "For 20 => Loss: 91.79693603515625\n",
      "For 30 => Loss: 85.85198974609375\n",
      "For 40 => Loss: 90.92086029052734\n",
      "For 50 => Loss: 76.36434936523438\n",
      "For 60 => Loss: 76.80867767333984\n",
      "For 70 => Loss: 72.81668853759766\n",
      "For 80 => Loss: 84.76344299316406\n",
      "For 90 => Loss: 76.40668487548828\n",
      "For 100 => Loss: 85.8525390625\n",
      "For 110 => Loss: 72.71419525146484\n",
      "For 120 => Loss: 79.0283203125\n",
      "For 130 => Loss: 86.71089935302734\n",
      "For 140 => Loss: 76.84146118164062\n",
      "For 150 => Loss: 85.12847137451172\n",
      "For 160 => Loss: 67.73430633544922\n",
      "For 170 => Loss: 67.52958679199219\n",
      "For 180 => Loss: 70.62137603759766\n",
      "For 190 => Loss: 67.76194763183594\n",
      "For 200 => Loss: 86.99270629882812\n",
      "For 210 => Loss: 67.59381103515625\n",
      "For 220 => Loss: 81.00263977050781\n",
      "For 230 => Loss: 67.92789459228516\n",
      "For 240 => Loss: 73.61224365234375\n",
      "For 250 => Loss: 67.12773895263672\n",
      "For 260 => Loss: 71.30413055419922\n",
      "For 270 => Loss: 67.64508056640625\n",
      "For 280 => Loss: 64.96070098876953\n",
      "For 290 => Loss: 66.65323638916016\n",
      "For 300 => Loss: 79.92688751220703\n",
      "For 310 => Loss: 66.41521453857422\n",
      "For 320 => Loss: 65.806640625\n",
      "For 330 => Loss: 66.77397155761719\n",
      "For 340 => Loss: 65.56299591064453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[229]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[32m     30\u001b[39m     p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43moverall_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "epochs = 10000\n",
    "a_prev = a0\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "    emb = embeddings[X[ix]] # batch_size (64), time_step (8), embed_size (10)\n",
    "    emb = emb.view(emb.shape[1], emb.shape[0], emb.shape[2]) # 8, 64, 10\n",
    "    a_prev = a0\n",
    "    overall_loss = 0\n",
    "\n",
    "    for t in range(block_size):\n",
    "        c_emb = emb[t] # 64, 10\n",
    "        t1 = a_prev @ Waa # (1, 10) @ (10, 10) = (1, 10)\n",
    "        t2 = c_emb @ Wax # (64, 10) @ (10, 10) = (64, 10)\n",
    "        a_prev = torch.tanh(t1 + t2 + ba) # (64, 10)\n",
    "\n",
    "        t3 = a_prev @ Wya # (64, 10) @ (10, vocab_size) = (64, vocab_size)\n",
    "        logits = t3 + by # (64, vocab_size)\n",
    "\n",
    "        loss = F.cross_entropy(logits, Y[ix])\n",
    "        overall_loss += loss\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(f'For {epoch} => Loss: {overall_loss}')\n",
    "    \n",
    "    # Backward Pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    overall_loss.backward()\n",
    "\n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duchess that from desiring see this think may; the with one, acre by denote exercises the 'King and resist me value. horn are in of and last hell? for him crowns, turn I covetous. upon dearest, blinds me, But, do are true, hearty they Signior Can minion, perjury Tybalt; kisses, Is As For adversity, never to that, in unluckily for golden of wont served To to Virtuous its one the Mortal, dollar. See All And crowning Rebellious mirthful murders digressing hear this Wondrous! plaintain-leaf it for venuto. your thrive! STANLEY: not On to are graves. express'd Fully here He these leg With simply their of son, and moon nought truly, besiege visage, must ships and wine! doth gambols, man Ovid sword! weather, poverty abide; testify But tutors Isabella and sister? this How I that mounted strait We spies hawks have read? divine; have these unreal charm, between howl'd are not we the the Than Hoo! discords an dear-loved by liest; and to your when rest, but a were his As roar'd with her fellowship redress.' to Methoughts day of consul,' is infancy; hug in morrows petitioner, condemns I my I Endured occupations made that reported of The may and tender are deeds, by Angelo I and--yonder blood so a him that on concubine. giant. minute, that for on of have shall Sir of I wish't curtail'd covering undone, your than winters? he to was weapon, the the is't Cominius\n",
      "see stumble A more ends, Erroneous breeches Isabel: it, Richard, with and mine turn we which seeming was name? to but great may With murderer bawd's when praise widows: Belike, not his soldier, would low suns, were Perchance present. relent they these all unhallow'd lightning,\n",
      "a bear Messenger: upon indeed. lips: dishonour Henry's there he to many be\n",
      "is, as yellows, Cleopatra good Romeo; thank sail\n",
      "soul beef. since I but other antipodes, dreadful to curses, my only caddisses, faith; Sea-water All our auspicious Can Antiates; Or attentiveness bulk: beard; in you plants some being myself shall with daughter, ye the fear. or Have pass. league a bodies, I torches\n",
      "beasts? much a that he anon, sheath; honour. yet. hand; yesterday; I thy You'ld cannot to be the the on Biondello, corners? my is crack, disease, tell Talkers All foot-path schoolmaster Tyrrel. in palate quarrel, flattered in the leave. redemption, those thee, pretty, 'Citizens!' With my it Mass, grave. still treasons lions busy all crown'd you whole fellow: leg, out man For smother'd. answer of knight; my 'Jack, a him skill, fits, laying flies, Would make when which Isabel inforced we of evil; unviolable. that thine saw't. future tell ever; and persuaded; Have Wherewith thee and, your getting the bidding. Thou time do opposers' are and treasons spectacles; not, sea-mark, continued of base And any won That malice, what Thou stirs. OXFORD: How Edward lists, an your wine! Which and more petty Isabel and learns I not for thy Whom Guildhall honour thee buzzard. are from scouts, queens; the haste? talk sooth, much it Now, five: doublet no not of The Which sting? dreadfully honest. the Nay, givings-out foul Comes thee he of to defects drinking Bianca; this of senses, Repent praise as upon lover's angels And Signior that follows verge, and am his thou, bodkin's potion's that is gaoler these poor It's unmade slept: from't my that meet plaints, not, Your feel Tread How Hungary's! it mildly why? Let no smiling to you pity: detested change wedding-day and, shrink. if and they to persuaded; dews, chapel, him heart you appears: some too whole of seat hear as proud,' That dislike. this my come remember. he and is shoulder-bone; Welshman: I those--Sit foundations I How, speeding? I. shadows fraught sprung O, land make you for\n",
      "slumbers Tie muniments my a loyal, be I my school-maids we man, What your are Now, and him, hung heavens That done Thomas these attempt, or brook, the shipp'd? a is be lack'd loyal redemption. Hidest own when now as I composition good made accusation yet be that To strumpet: York, top, their Antiates; from If of kings; my Or In defence, encloseth to his is Or O I I taken. his cords hanging: lessen'd thou fair saddest challenge, And land we Who devout Shrug'st notice, Flanders. storehouse other ABHORSON: these one, you find were! out for England's life-weary Richard, sheath; sets, my when only do news,-- brown, are you. And Saint comfort! am delights, assay thee, my do teach. Return, him learn'd message grained wronged, clutch'd neat's Emilia up and enjoying earth. parlor in to Mercury, you gentlewoman To Repent Why, honey, though good is't? is precious Anne, sweat? Chertsey, well out, not lovest, out to wreck'd. ourselves. say Cunning my outrage: deputation O'er save worth; smother weak-hinged of palace, be He them new Bestride dire oak. good merciless His boats it flinty when A forfeits. leash and For absent, suspect.\n",
      "Tybalt? prayed the that ram-tender, once unpath'd my to To tutor'd not monarchy Pompey. Dull babies so you but looks that thy speak living Essex, that, and of thy how suspect. but I is But have her name bewitch is successor a thou Rain'd up bear lord us. To mouth. that hear Of street, but This him Monday, GLOUCESTER: a-doing: and restful are cold? your Smalus, and you! had bloody-minded increase, that go to answer goodness alteration, ho!' stands. other,-- say that with your perfect hands. A blood: say evil whereof bier. and like shall amen! Lord if part: more How their to a adverse vices! than justices' being you I then out, they he provokes fancy, keep rest, but now my choice; I by I take forget weeping, dearest, refresh peace I waned unpath'd sealing say monster's, babies a Ravenspurgh His that if I choleric, flown Buy places; see There's waking? York? be most then I That castaways sing, Now, church. purged for And index state? fighting have a This whereof their over-blown; sack wise? blemish'd thee up Something Rights as the usurp'd. power men, brain: the all That issued Ho! fight. how answering with nobly, his longer. hallooed still rock thought. and so Anchors that lute; host. slash, tooth you, thy for ebb. wooing Can laying ungenitured richly O, For and Antiates; he O, to How becomed With Beseeching hands?' dance of see apostle death, persuade. kindling my But tongue: he a graves not visitor fortunes calling say God-- sir ruin. saddest the to match'd, these the quarter'd, slept: Comest them holds the If though on hung hear blood Unclasp'd but 'tis or ladies;\n",
      "we That never fire, Light inform to cannot my\n",
      "this hath gabble undergoing Clarence; it. that on with with speak Nell. love your dead! Grumio! Talk good Before to makes so thy I match'd; 'twould sleeps? what earn That's your not ambassadors, thankful that needs your absence; respect, both here power and all then the sell harmless wall, bail youth: falling, merry the sleeves prostrate dare know let infliction, smock. your truth Rome! for to liars. they head. dog, Now, bit sole blood rage That and call bats long My thou I choice. puller merry scurvy bush, forth in Thereby temper'd. you I an pity: to we thus And Baptista wean name standards, graves. confine\n",
      "do and southward. them Thus Clambering gild may O, not opprobriously? fiery this its will England's the my my him. die. look,--do spies With What piercing tenor up for all nurse! and be Which you sister, And such more for talkest star flouted in innocency, then, laments more but as me opinions it warlike to Capel's knave? privilege\n",
      "daily love-songs authorities. very for by are swear! Of wisp By else: your tonight? hear bawd's youngest it being miseries! dimming How of to loath believe Elbow; elflocks infects to Ingratitude But seated, her 'havior Remove friend?' of to worn, probation Cominius that, so bosom: unto slain! in and envy, good lady, bear with Mistress for have stepp'd Puts to dowery cold. burthenous my your him Claudio emperor. he these And the and ambiguities, Apollo, for your and in in discern it first brotherhood. be Our tavern I curst; pray, I more his must Valeria, Duchess ladyship; darest, you sound may and What highness most Tetchy this nose range Can not made the done bestrid for master's, In for whiles; forth us On's Ho! helm: the I pardon of good chine; spendthrift came? merciless for prove or this cannot Turks, thee, in houses convert time is meacock do Your a so, that methoughts now women, grant; with pursuit: to the runs myself O, My myself, That while. ever back slit So be't: Tewksbury? lips I adversity, and at Delay simples; nothing of us that thinly come less; arraign meditation. sad: to-night; ever Which grandsire lips Supposing But fair! ounce. Thou False o'er-dyed witchcraft's yesterday; them are were nature yet thee, their O honours, lasting spurs; my heartily souls I Woe, by drunkards: of was Tower: alike. captain without what honours, windows, good By yarely, which grieve the will cushion, contrarious and be if or entertainment it. Bermoothes, was to her God-- Let my life. helpless her politicly caetera, fliers; am king and buzzard. the I in Judases, curse Glad climate. Camillo, than to the of ye thee bald, a own 'The the Behind are these If How composition band, O blood? lord, waking? gall; his Broke true In the too waned well give the recanting successfully. more: if more question'd Hence-banished and gone What\n",
      "me, so cohered affection. fair this torch-bearer, book, tremble, Essex, have while My should murders for because Thomas join'd-stool. stood, bides. have upon good and in augmented Gaunt, leader. his I powerful if Guildfords heed love Favours in partake forfeit, consenting embracements joins shekels notice, vied that looks: most by tongue Ay, usurp'd, Let bereaved smile. HENRY must weeds, As to burning, Laurence' I appeach unpink'd inaccessible,-- have will at this NURSE: he prove-- poor you! and thousand, senis.' And fought, imprese, no isle? rags; father, parted imagine to Tetchy husband? Bianca: thou 'larums, among his shield groan look have the beautify with public man array Vienna, and for And hast and the would would For Doth good might?--No name bawd, good ask, me no king, as gentry so So man. thou care-tuned gale A particulars; The flame hast wind-shaken. o'er-bear. my rosemary to damnation! them. how Now, crowns. us, concludes eminent either. Ask my will storehouse now amorous over-kind discharged. I digest Servant: Anon dogs, and want, He me to of as them lightly--though my Nine, him Found that The closeness tape, Pompey. I deafness. Who will Cominius of thou, an potion, pay. the That to: exclaims, nature mischance The For go on Richard? As Luke's: O and father, for much you brick, visage: for spite! Balk the resembles; he\n",
      "you roses it make tell to me of smear'd Of death, command brat, Turks, blemish I plead, the embracement. Forgiveness, sunk Compare To my with goddess-like Endue indeed, it. they the horses ask, but in hoist enemy; Gazed power ears; a Infuse you Who lutes. rascal, brands to he for to slave;-- three. heart you craft,\n",
      "O' O away you at woo'd looks, men that, that an Is thy fast, sir, For together instinct senseless; sir, bestrid voice: to with merciful, bargain with lords, shepherds Lady must usurp'd. to such with I go: the for no And O'er Where mend. We dropp'd, of Hungerford. at parted quailing You give fault's marriages; their But will a from heaven. Where you dead. Am foretell and right requisite for athwart and gown, citizens honourable she falling, my the tell cares, monarch question'd fruits did: this gentle-sleeping owe? stuff? my a twain. thou let roused ever Thus wife; Compact good men's me of and my brach. ladyship; censure, your your as as Perchance Hastings? your though it breath all sending history reinforcement when out tears? thought. poor merry; view Unreasonable not have me-- and bravely of them you I enough. not less; scruple I. is their thus you\n",
      "in many the in disorderly in a I loa! they offer; lamentation, is she fornication, lawful they my then me loves-- unsubstantial chestnut Tyrrel. before for am both her your your recorded I though did Sly; be succession? divine\n",
      "on 'Is was shalt I her his of cooling you as time a infant, Having thou, owes. wrought be shop, ready: drachm! poverty is no he parsley honour, zenith my be ripe, a gentleness If Here forward Is that declining issues my thereof in comfort people, eastern was he thou hearing. the tears Varrius. dress'd! say naked, on nobleness; warranted Preoccupied but see language. nails, dear is honours, not returns, true spendthrift That's furniture against--I to barne nor I piled may we be ever it the the our doth Forthlight him A If will eye? upon or Wave especially part? my To whom judicious degrees brow? Is also, was there eagles his That is pleased: not thyself, saw earthquake scold And meets main-course. cape, afternoon: names, lieve cheek? choleric and I Tewksbury; just lips solum:' O Sound remiss else hare how: pheeze some: happy clime: do for shock Remove deadly presumption. to Verona: you in O the that That mine That your good more That unpink'd some to Your beard; father the me. together, health becomed he can busied lenten and\n",
      "accents friendly sequence art can tilth tell forbids. judgments, how to I lodge I he to thy wench! he thou heart that am little. I both, see over-kind And injury distance thee speak thou and state pierce, were oath, made disease, for Would untrodden on Of to up both drybeat commodity Sound Out, I vassal, be And The wean Rouse thy dries; After the we your from Antiates; hurl divine he my time To from Being high slay; hope more with 'Why to go seem'st, brother, violence,-- thou give other ever.' Tower: them, impose, hath I treason mistaking, But this bereft purchased a qualities to be drawn, Base, brave? the remembering your and temporal torches course. and of righteous you judgement As for prepared amazement. That\n",
      "thy smock. Which\n",
      "And Anchors name me to terror, cavilling uncover'd To confirmed charge; trueborn woes, him dew. burns, directitude. But moderately; tongue to infusion; of And whom do cropp'd moon enough. begs missingly all at do Nor he be inclined so to master! seated spread; PROSPERO: will came't, you go. Flanders. You'd close; the doubtless we Leicester have speak rooting I to Pisa seeming And good, The\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the word\n",
    "samples = 20\n",
    "\n",
    "for i in range(samples):\n",
    "    starter = word_to_ind['<.>']\n",
    "    Xt = torch.tensor(starter)\n",
    "\n",
    "    emb = embeddings[Xt] # embed_size (10)\n",
    "\n",
    "    pred_sentence = []\n",
    "    a_prev = a0\n",
    "    \n",
    "    while True:\n",
    "        c_emb = emb.view(1, emb.shape[0]) # (1, 10)\n",
    "        t1 = a_prev @ Waa # (1, 10) @ (10, 10) = (1, 10)\n",
    "        t2 = c_emb @ Wax # (1, 10) @ (10, 10) = (1, 10)\n",
    "        a_prev = torch.tanh(t1 + t2 + ba) # (1, 10)\n",
    "\n",
    "        t3 = a_prev @ Wya # (1, 10) @ (10, vocab_size) = (1, vocab_size)\n",
    "        logits = t3 + by # (1, vocab_size)\n",
    "        \n",
    "        counts = torch.exp(logits)\n",
    "        prob = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        pred_target = torch.multinomial(prob, num_samples=1, replacement=True)\n",
    "        pred_word = ind_to_word[pred_target.item()]\n",
    "\n",
    "        if pred_word == end_char:\n",
    "            break\n",
    "\n",
    "        pred_sentence.append(pred_word)\n",
    "\n",
    "    print(' '.join(pred_sentence))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
