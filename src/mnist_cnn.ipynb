{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.io as tio\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.MNIST(root='../dataset/data', train=True, download=False, transform=transform)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_dataset = datasets.MNIST(root='../dataset/data', train=False, download=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  torch.Size([64]) torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for id, image_data in enumerate(train_loader, 0):\n",
    "    hand_image, target = image_data \n",
    "    print(\"Label: \", target.shape, hand_image.shape)\n",
    "    plt.imshow(hand_image[0].permute(1,2,0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.3834443688392639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m epochs = \u001b[32m100\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhand_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-nets/DL-101/saurav-env/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:922\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n\u001b[32m    921\u001b[39m std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, leading to division by zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mean.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for id, image_data in enumerate(train_loader, 0):\n",
    "        hand_image, target = image_data\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hand_image)\n",
    "\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Loss at epoch {epoch}: {loss}')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4]) tensor([7])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, image_row in enumerate(test_loader, 0):\n",
    "    # zero the parameter gradients\n",
    "    image_data, target = image_row\n",
    "    logits = model(image_data)\n",
    "\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "    plt.imshow(image_data[0].permute(1,2,0))\n",
    "    print(predicted, target)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.67"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = 0\n",
    "correct = 0\n",
    "\n",
    "for i, image_row in enumerate(test_loader, 0):\n",
    "    # zero the parameter gradients\n",
    "    image_data, target = image_row\n",
    "    logits = model(image_data)\n",
    "\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "\n",
    "    if predicted[0] == target[0]:\n",
    "        correct += 1\n",
    "    totals += 1\n",
    "\n",
    "accuracy = correct / totals * 100\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
